# 데이터 파이프라인에서 Apache Airflow를 사용한 이유

Apache Airflow는 워크플로우를 스케줄링하고 모니터링하며, 데이터 파이프라인의 복잡한 작업들을 효과적으로 관리할 수 있도록 설계된 오픈소스 플랫폼입니다. 이 문서에서는 프로젝트에서 Airflow를 선택한 이유를 정리하였습니다.

---

## 1. 워크플로우의 선언적 코드화
Airflow는 워크플로우를 Python 코드로 작성할 수 있어, 선언적이고 유연한 방식으로 작업 흐름을 정의할 수 있습니다.
- 코드 기반으로 워크플로우를 정의하므로 **버전 관리**가 용이
- 복잡한 데이터 파이프라인의 시각적 표현 가능

---

## 2. 강력한 스케줄링 기능
Airflow는 **크론 스타일**의 스케줄링 및 특정 조건 기반의 트리거를 지원하여 데이터 파이프라인의 실행 시점을 세밀하게 조정할 수 있습니다.
- 정기적인 데이터 로드 작업 자동화
- 이벤트 기반 워크플로우 실행 가능

---

## 3. DAG(Directed Acyclic Graph) 기반 설계
Airflow는 DAG(Directed Acyclic Graph)로 작업을 정의하여 다음과 같은 이점을 제공합니다:
- 작업 간의 **의존성 관리**
- 순차적 및 병렬 작업 처리
- 데이터 파이프라인의 단계별 실행 상태 추적

---

## 4. UI를 통한 직관적인 모니터링
Airflow는 웹 기반 UI를 통해 워크플로우 상태를 실시간으로 모니터링할 수 있습니다.
- 작업 실행 상태(성공, 실패, 진행 중) 확인
- 재실행, 로그 조회, 작업 히스토리 관리
- DAG의 실행 흐름을 그래프로 시각화

---

## 5. 유연한 확장성 및 다양한 연동
Airflow는 다양한 데이터베이스, 클라우드 스토리지, API와 쉽게 통합됩니다.
- 플러그인 및 커넥터를 통해 확장 가능
- AWS, GCP, Azure 등 주요 클라우드 서비스와 통합 가능
- 데이터베이스, Spark, Hadoop 등과의 연동을 지원하여 **ETL 작업 최적화**

---

## 6. 오류 관리 및 재실행
Airflow는 오류 발생 시 작업을 재실행하거나 특정 작업부터 다시 시작할 수 있는 기능을 제공합니다.
- 실패한 작업만 재실행하여 시간 절약
- DAG 실행 중단 후 특정 지점에서 재개 가능

---

## 7. 확장 가능한 아키텍처
Airflow는 분산 환경에서 실행 가능하며, 여러 워커 노드를 활용하여 대규모 작업을 처리할 수 있습니다.
- 병렬 작업 처리
- 확장 가능한 클러스터 아키텍처

---

## 📌 예시 응용 시나리오
1. **정기적인 데이터 수집**  
   - API 호출, 데이터베이스 쿼리 등을 통해 정기적으로 데이터를 수집하고 저장

2. **ETL(Extract, Transform, Load)**  
   - 데이터를 추출하여 변환 후 데이터 웨어하우스 또는 데이터 레이크에 적재

3. **데이터 품질 검사**  
   - 데이터 품질을 자동으로 검사하고 이상 탐지 시 알림 전송

4. **머신러닝 파이프라인**  
   - 모델 학습, 배포 및 검증 과정을 자동화

---

## 결론
Apache Airflow는 유연한 워크플로우 정의, 강력한 스케줄링, 다양한 데이터 소스 연동, 직관적인 UI를 제공하여 데이터 파이프라인 관리 및 자동화를 효과적으로 지원합니다.  
이를 통해 데이터 파이프라인의 복잡도를 줄이고, 운영 효율성을 극대화할 수 있습니다.
