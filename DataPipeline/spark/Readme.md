# 데이터 파이프라인에서 Apache Spark를 사용한 이유

Apache Spark는 대규모 데이터를 빠르게 처리하기 위한 분산 데이터 처리 플랫폼입니다. Spark는 다양한 데이터 처리 요구사항을 하나의 프레임워크로 통합하여 데이터 엔지니어링, 데이터 분석, 머신러닝 작업 등을 효과적으로 지원합니다. 이 문서에서는 프로젝트에서 Spark를 선택한 이유를 정리하였습니다.

---

## 1. 빠른 데이터 처리
Apache Spark는 인메모리 컴퓨팅을 활용하여 데이터를 빠르게 처리할 수 있습니다.
- 디스크 I/O를 최소화하여 **높은 성능** 제공
- 분산 환경에서 데이터를 병렬 처리하여 작업 속도 향상

---

## 2. 다양한 데이터 처리 작업 지원
Spark는 다음과 같은 데이터 처리 작업을 지원하며, 하나의 플랫폼에서 통합적으로 사용할 수 있습니다.
- **배치 처리**: 정적 데이터의 대규모 분석 작업
- **스트리밍 처리**: 실시간 데이터 스트림 처리
- **SQL 처리**: 구조화된 데이터에 대한 SQL 쿼리 실행
- **머신러닝**: MLlib 라이브러리를 통해 확장 가능한 머신러닝 모델 제공
- **그래프 처리**: GraphX를 활용한 그래프 데이터 분석

---

## 3. 확장성과 분산 아키텍처
Spark는 클러스터 기반의 분산 아키텍처를 활용하여 대규모 데이터를 처리할 수 있습니다.
- **확장성**: 클러스터에 노드를 추가함으로써 작업량 증가에 유연하게 대응
- **분산 처리**: 데이터와 작업을 여러 노드로 분산하여 효율적인 처리 가능

---

## 4. 다양한 데이터 소스 연동
Spark는 다양한 데이터 소스와의 연동을 지원합니다.
- 데이터베이스: MySQL, PostgreSQL 등
- 클라우드 스토리지: AWS S3, GCP Cloud Storage 등
- 데이터 레이크: HDFS, Delta Lake 등
- 스트리밍 데이터: Kafka, Flume 등

---

## 5. 유연한 프로그래밍 인터페이스
Spark는 Python, Scala, Java, R 등의 언어를 지원하여 개발자가 익숙한 언어로 작업할 수 있습니다.
- PySpark를 통해 Python 기반의 데이터 처리 가능
- 다양한 언어로 동일한 작업을 처리할 수 있어 팀 간 협업에 유리

---

## 6. 커뮤니티와 확장성
Spark는 활발한 오픈소스 커뮤니티를 가지고 있으며, 플러그인과 라이브러리를 통해 기능을 확장할 수 있습니다.
- MLlib, GraphX 등 내장 라이브러리를 활용한 고급 분석 작업 가능
- Spark 외부 라이브러리를 통합하여 특정 작업에 최적화 가능

---

## 7. Fault Tolerance(내결함성)
Spark는 작업이 실패하더라도 DAG(Directed Acyclic Graph)를 활용하여 실패한 작업만 재실행할 수 있습니다.
- **Resilient Distributed Dataset(RDD)**로 데이터를 복구 가능
- 클러스터 노드 장애 발생 시에도 작업을 중단하지 않고 진행 가능

---

## 📌 예시 응용 시나리오
1. **배치 처리**  
   - 대규모 로그 데이터를 분석하여 사용자 행동 패턴 추출
2. **스트리밍 처리**  
   - Kafka에서 실시간으로 데이터를 수신하여 데이터 변환 및 저장
3. **데이터 변환**  
   - 데이터 정규화, 필터링, 집계 등 데이터 ETL(Extract, Transform, Load) 작업
4. **머신러닝 파이프라인**  
   - Spark MLlib을 사용하여 대규모 데이터 학습 및 예측
5. **데이터 시각화 준비**  
   - 데이터를 변환하여 BI 도
