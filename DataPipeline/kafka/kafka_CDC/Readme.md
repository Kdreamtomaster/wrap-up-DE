
# 데이터 파이프라인 관점에서 Kafka를 사용한 CDC(Change Data Capture)

Kafka는 고성능 분산 메시징 시스템으로, 데이터 파이프라인에서 CDC(Change Data Capture)를 구현하는 데 적합한 도구입니다. 이 문서에서는 Kafka CDC의 개념과 데이터 파이프라인 관점에서 이를 사용하는 주요 이유를 설명합니다.

---

## 1. CDC(Change Data Capture)란 무엇인가?

CDC는 데이터베이스에서 발생하는 변경 사항(삽입, 업데이트, 삭제)을 캡처하여 실시간으로 다른 시스템에 전달하는 기술입니다.
- 주로 데이터 동기화, 실시간 데이터 처리, 이벤트 기반 아키텍처에 활용됩니다.
- 데이터베이스의 로그(binlog, WAL 등)를 읽어 변경 내용을 스트리밍 방식으로 전달합니다.

---

## 2. Kafka를 CDC에 사용하는 이유

### 2.1 고성능 및 확장성
- Kafka는 **분산 아키텍처**를 기반으로 설계되어 높은 처리량과 확장성을 제공합니다.
- 대규모 데이터베이스 변경 사항을 처리하는 데 적합하며, 수평 확장을 통해 성능을 쉽게 향상시킬 수 있습니다.

### 2.2 낮은 지연시간
- Kafka는 실시간 스트리밍 데이터 처리를 지원하므로, 데이터베이스 변경 사항을 거의 실시간으로 소비자에 전달할 수 있습니다.

### 2.3 내구성 및 신뢰성
- Kafka의 로그 기반 저장소는 변경 데이터를 디스크에 영구적으로 저장하여 장애 복구 및 데이터 손실 방지에 탁월합니다.
- **데이터 재처리 가능**: 소비자가 데이터를 놓친 경우, Kafka에서 오프셋(offset)을 기반으로 다시 데이터를 가져올 수 있습니다.

### 2.4 다양한 데이터 소비자 지원
- Kafka는 다양한 데이터 소비자에게 데이터를 스트리밍할 수 있어 **다대다(M:N)** 데이터 파이프라인을 구현할 수 있습니다.
- 예: 데이터베이스 변경 사항을 데이터 웨어하우스, 분석 시스템, 캐시 시스템 등에 전달.

### 2.5 이벤트 중심 데이터 처리
- Kafka는 데이터베이스 변경 사항을 이벤트로 간주하여 이벤트 기반 아키텍처를 구현하는 데 적합합니다.
- 데이터베이스 트랜잭션 로그를 스트림으로 변환하여 실시간 분석 및 트리거 기반 워크플로우 실행 가능.

---

## 3. Kafka CDC의 주요 구성 요소

### 3.1 Kafka Connect
- 데이터 소스와 Kafka 간의 데이터 이동을 간소화하는 플러그인 기반 프레임워크.
- **CDC 커넥터**: 데이터베이스 로그를 읽어 Kafka 토픽으로 전송.
  - Debezium과 같은 오픈소스 CDC 커넥터를 활용 가능.

### 3.2 Kafka 토픽
- CDC 데이터를 저장하고 전달하는 논리적 채널.
- 데이터베이스 변경 사항을 토픽으로 분류하여 다양한 소비자가 구독 가능.

### 3.3 소비자(Consumer)
- Kafka에서 전달된 데이터를 소비하여 데이터 웨어하우스, 분석 시스템, 캐시 시스템 등으로 전달.

---

## 4. 데이터 파이프라인 관점에서 Kafka CDC의 장점

### 4.1 실시간 데이터 동기화
- 데이터베이스의 변경 사항을 실시간으로 다른 시스템과 동기화 가능.
- 예: MySQL에서 변경된 데이터를 실시간으로 Elasticsearch에 반영.

### 4.2 복잡한 데이터 파이프라인 간소화
- Kafka CDC를 사용하면 데이터 파이프라인의 복잡성을 줄이고 효율성을 높일 수 있습니다.
- 동일한 CDC 데이터를 여러 소비자가 구독하여 다양한 작업(분석, 캐싱, 데이터 적재 등)을 수행 가능.

### 4.3 데이터 흐름의 중앙화
- Kafka는 데이터 변경 사항의 **중앙 허브** 역할을 수행하여 데이터 흐름을 일원화.
- 데이터 소스와 소비자 간의 느슨한 결합(Decoupling) 구조를 통해 확장성 향상.

### 4.4 데이터 재처리 지원
- Kafka는 오프셋 기반 데이터 재처리를 지원하므로, 소비자가 데이터를 놓쳤거나 처리 로직을 변경해야 할 경우 유용.

### 4.5 다양한 데이터 소스와의 통합
- Kafka Connect를 통해 MySQL, PostgreSQL, MongoDB 등 여러 데이터베이스와 쉽게 통합 가능.

---

## 5. 사용 사례

### 5.1 데이터 동기화
- 운영 데이터베이스와 데이터 웨어하우스 간 실시간 데이터 동기화.

### 5.2 실시간 분석
- 데이터베이스 변경 사항을 스트리밍 데이터로 처리하여 실시간 분석 수행.

### 5.3 이벤트 기반 워크플로우
- 데이터베이스에서 발생하는 이벤트(예: 주문 생성, 결제 완료)를 기반으로 비즈니스 프로세스 자동화.

### 5.4 로그 기반 데이터 처리
- 데이터베이스 트랜잭션 로그를 Kafka로 전송하여 로그 분석 및 아카이빙.

---

## 6. 결론

Kafka CDC는 데이터 파이프라인에서 실시간 데이터 처리와 데이터 동기화를 구현하는 강력한 도구입니다.  
이를 사용하면 데이터의 흐름을 중앙화하고, 데이터베이스와 분석 시스템 간의 통합을 간소화하며, 확장성과 신뢰성을 보장할 수 있습니다.  
데이터 파이프라인에서 CDC를 구현할 때 Kafka는 성능과 유연성 면에서 뛰어난 선택이 될 수 있습니다.
